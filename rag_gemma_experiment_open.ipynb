{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0e5249c625994e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:38:27.902159Z",
     "start_time": "2024-04-10T21:38:19.900137Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Keith_McNulty/PycharmProjects/rag_gemma_regression_book/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e0f094428efea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Retrieval-Augmented Generation (RAG) Using Google Gemma-7b-it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9722825b18d9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this notebook we will implement the following RAG architecture to create a customized LLM using text from my textbook **Handbook of Regression Modeling** and Google's Gemma-7b-it open source LLM:\n",
    "\n",
    "![RAG Architecture](rag_architecture.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b03658dd838d5e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will be building two components, an information-retrieval (IR) component and a generation component:\n",
    "* The IR component acts as a knowledge database of text files.  This database will be used to identify documents or text passages most relevant to the intent of the user query. Embeddings will be used to search this database and return the most relevant passages from the textbook.\n",
    "* The generation component will feed the results of the IR component into the Gemma LLM as context in order to generate an attempt at a comprehensive natural language response to the user prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed1be801881cd6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preparing a dataset from my textbook to work with Gemma-7b-it LLM\n",
    "\n",
    "My textbook is available open source, and its codebase is in an open Github repository.  We will connect to the repository and download the text of the 14 chapters and sections of the textbook, and create a Pandas dataframe with 14 rows and two columns, containing an ID-number for each chapter/section and the text of each chapter/section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd3a95ff95c043",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:38:39.987284Z",
     "start_time": "2024-04-10T21:38:36.088369Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# chapters are Rmd files with the following names\n",
    "chapter_list = [\n",
    "    \"01-intro\",\n",
    "    \"02-basic_r\",\n",
    "    \"03-primer_stats\",\n",
    "    \"04-linear_regression\",\n",
    "    \"05-binomial_logistic_regression\",\n",
    "    \"06-multinomial_regression\",\n",
    "    \"07-ordinal_regression\",\n",
    "    \"08-hierarchical_data\",\n",
    "    \"09-survival_analysis\",\n",
    "    \"10-tidy_modeling\",\n",
    "    \"11-power_tests\",\n",
    "    \"12-further\",\n",
    "    \"13-solutions\",\n",
    "    \"14-bibliography\"\n",
    "]\n",
    "\n",
    "# create a function to obtain the text of each chapter\n",
    "def get_text(chapter: str) -> str:\n",
    "    # URL on the Github where the rmd files are stored\n",
    "    github_url = f\"https://raw.githubusercontent.com/keithmcnulty/peopleanalytics-regression-book/master/r/{chapter}.Rmd\"  \n",
    "    \n",
    "    result = requests.get(github_url)\n",
    "    return result.text\n",
    "\n",
    "# iterate over the chapter URLs and pull down the text content    \n",
    "book_text = []\n",
    "for chapter in chapter_list:\n",
    "    chapter_text = get_text(chapter)\n",
    "    book_text.append(chapter_text)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# write to a dataframe\n",
    "book_data = dict(chapter = list(range(14)), text = book_text)\n",
    "book_data = pd.DataFrame.from_dict(book_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:38:41.133216Z",
     "start_time": "2024-04-10T21:38:41.130380Z"
    }
   },
   "id": "1a1a682fe90ec35d",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "b179c9d6bcdc9cb2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Most of these text documents are too long to fit into Gemma's context window, and so we will need to split them into smaller documents in a way that makes some sort of semantic sense.  \n",
    "\n",
    "We will use a Langchain transformer to do semantic splitting, with a chunk size of 1000 and a chunk overlap of 150. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e43e94c91caa2b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:38:45.562841Z",
     "start_time": "2024-04-10T21:38:45.511722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Document(page_content=\"`r if (knitr::is_latex_output()) '\\\\\\\\mainmatter'`\\n\\n# The Importance of Regression in People Analytics {#inf-model}\\n\\nIn the 19th century, when Francis Galton first used the term 'regression' to describe a statistical phenomenon (see Chapter \\\\@ref(linear-reg-ols)), little did he know how important that term would be today.  Many of the most powerful tools of statistical inference that we now have at our disposal can be traced back to the types of early analysis that Galton and his contemporaries were engaged in.  The sheer number of different regression-related methodologies and variants that are available to researchers and practitioners today is mind-boggling, and there are still rich veins of ongoing research that are focused on defining and refining new forms of regression to tackle new problems.\", metadata={'chapter': 0})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# semantically split chapters to a max length of 1000\n",
    "loader = DataFrameLoader(book_data, page_content_column=\"text\")\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "# examine a document to ensure it looks as we expect\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d19a3123e9bc1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generating embeddings and storing in a vector DB\n",
    "\n",
    "![Embeddings](embeddings.png)\n",
    "\n",
    "Since we will use embeddings for the IR component, we need to generate embeddings for our split dataset and then write those into a vector database to allow them to be searched.  We will use the *all-MiniLM-L6-v2 model* to generate the embeddings and the ChromaDB vector database to store them.  Vector Databases have limits to the number of documents that can be encoded in a single command, and so we will use a batch command just in case there are too many documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e18358ebb839a25",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:38:50.061922Z",
     "start_time": "2024-04-10T21:38:49.658957Z"
    }
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.utils.batch_utils import create_batches\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0fd8b8d2b3b4bc3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:40:12.815552Z",
     "start_time": "2024-04-10T21:40:12.804359Z"
    }
   },
   "outputs": [],
   "source": [
    "# set up the ChromaDB\n",
    "CHROMA_DATA_PATH = \"./chroma_data_regression_book/\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "COLLECTION_NAME = \"regression_book_docs\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)\n",
    "\n",
    "# in case docs have already been written\n",
    "client.delete_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48eb1e0aeeac6899",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:40:22.697007Z",
     "start_time": "2024-04-10T21:40:18.681604Z"
    }
   },
   "outputs": [],
   "source": [
    "# enable the DB using Cosine Similarity as the distance metric\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBED_MODEL\n",
    ")\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_func,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d63b79fb40377f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:40:35.652792Z",
     "start_time": "2024-04-10T21:40:28.306940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding batch of size 578\n"
     ]
    }
   ],
   "source": [
    "# write text chunks to DB in batches \n",
    "batches = create_batches(\n",
    "    api=client,\n",
    "    ids=[f\"{uuid.uuid4()}\" for i in range(len(docs))], \n",
    "    documents=[doc.page_content for doc in docs], \n",
    "    metadatas=[{'source': './handbook_of_regression_modeling', 'row': k} for k in range(len(docs))]\n",
    ")\n",
    "\n",
    "for batch in batches:\n",
    "    print(f\"Adding batch of size {len(batch[0])}\")\n",
    "    collection.add(ids=batch[0],\n",
    "                   documents=batch[3],\n",
    "                   metadatas=batch[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c54d6d4810edee",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now our documents are persisting in our vector database, we can try running a query against them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b34776f4ec93aa9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:40:37.078099Z",
     "start_time": "2024-04-10T21:40:37.061121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'ids': [['371eeda2-c01f-420b-80c6-2b61894d0069',\n   '94fc3891-4b37-49b9-a797-e73a11eec739',\n   '3360431f-6857-47ac-a329-af10b6528c61']],\n 'distances': None,\n 'metadatas': None,\n 'embeddings': None,\n 'documents': [['# Proportional Odds Logistic Regression for Ordered Category Outcomes {#ord-reg}',\n   \"# Multinomial Logistic Regression for Nominal Category Outcomes\\n\\n`r if (knitr::is_latex_output()) '\\\\\\\\index{multinomial logistic regression|(}'`\\nIn the previous chapter we looked at how to model a binary or dichotomous outcome using a logistic function.  In this chapter we look at how to extend this to the case when the outcome has a number of categories that do not have any order to them.  When an outcome has this nominal categorical form, it does not have a sense of direction.  There is no 'better' or 'worse'&zwj;, no 'higher' or 'lower'&zwj;, there is only 'different'&zwj;.\\n\\n## When to use it\\n\\n### Intuition for multinomial logistic regression \\n\\nA binary or dichotomous outcome like we studied in the previous chapter is already in fact a nominal outcome with two categories, so in principle we already have the basic technology with which to study this problem.  That said, the way we approach the problem can differ according to the types of inferences we wish to make.\",\n   'In fact, there are numerous known ways to approach the inferential modeling of ordinal outcomes, all of which build on the theory of linear, binomial and multinomial regression which we covered in previous chapters.  In this chapter, we will focus on the most commonly adopted approach: *proportional odds* logistic regression.  Proportional odds models (sometimes known as constrained cumulative logistic models) are more attractive than other approaches because of their ease of interpretation but cannot be used blindly without important checking of underlying assumptions.  \\n\\n## When to use it\\n\\n### Intuition for proportional odds logistic regression {#ord-intuit}']],\n 'uris': None,\n 'data': None}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"Which method would you recommend for ordered category outcomes?\"],\n",
    "    n_results=3, \n",
    "    include=['documents']\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae92b4efc3aa9c5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pipelining the RAG using ChromaDB and Gemma-7b\n",
    "\n",
    "We now have our vector DB in place so our IR layer is complete.  Now we will load the Gemma-7b LLM via Huggingface.  An access token is needed for this.  This model is large and will take some loading time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d88bc879e5425f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:40:53.680348Z",
     "start_time": "2024-04-10T21:40:53.569786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/Keith_McNulty/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"MPS not available\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:40:56.109938Z",
     "start_time": "2024-04-10T21:40:56.008691Z"
    }
   },
   "id": "7fe40f157bd70226",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "892768b23836f5ac",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:41:03.398229Z",
     "start_time": "2024-04-10T21:40:58.917025Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff1dca0191eb491ea6f16988d35af8b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model to Apple Silicon\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\", padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599083e8bfc4f824",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we have loaded the Gemma-7b-it model, we can set up an LLM pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can run the model normally and ask a general question to the Gemma model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2591706286ae702"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"New Mexico is known for its unique cuisine, blending Native American, Spanish, and Mexican influences. Here are some must-try foods in the Land of Enchantment:\\n\\n**Traditional Native American Foods:**\\n\\n* **Indian tacos:** Corn tortillas filled with meat (often mutton or beef), beans, cheese, lettuce, tomato, and red chili powder.\\n* **Chiles en nogada:** Layers of red and green chiles, potatoes, and vegetables in a savory sauce.\\n* **Sopaipillas:** Puffy fried dough balls often served with honey or jam.\\n* **Posole:** Stewed pork in a flavorful broth, served with corn tortillas and red chili powder.\\n\\n**Spanish and Mexican Influences:**\\n\\n* **Hatch chiles:** Green and red chiles grown in Hatch, New Mexico, known for their unique flavor and heat.\\n* **Red and green chile stew:** A hearty stew made with red and green chiles, vegetables, and meat.\\n* **Carne adovada:** Slow-roasted beef marinated in red chile powder.\\n* **Biscochitos:** Crispy fried dough cookies dusted with cinnamon and sugar.\\n* **Sopapillas:** A sweet or savory treat made with a fried dough pastry filled with fruit, honey, or cheese.\\n\\n**Other Must-Try Foods:**\\n\\n* **Green chile cheeseburger:** A twist on the classic cheeseburger, with green chile instead of red.\\n* **Blue corn:** A unique variety of corn with a deep blue color, often used in tortillas and other dishes.\\n* **Turquoise ice cream:** A refreshing ice cream made with turquoise-colored ice cream powder.\\n\\n**Additional Tips:**\\n\\n* **Consider your spice tolerance:** New Mexican cuisine is known for its spiciness. If you're not used to eating spicy food, be sure to ask for mild versions.\\n* **Try a local restaurant:** There are many great local restaurants in New Mexico that offer traditional cuisine.\\n* **Visit a food festival:** New Mexico has a number of food festivals throughout the year.\\n* **Be sure to try the local beer:** New Mexico has a thriving craft beer scene.\\n\\n**Enjoy your culinary journey in the Land of Enchantment!**\""
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<start_of_turn>user\n",
    "What food should I try in New Mexico?<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "# embed the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# generate the answer\n",
    "outputs = model.generate(**input_ids, max_new_tokens=512)\n",
    "\n",
    "# decode the answer\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True).split('model\\n', 1)[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:43:56.823075Z",
     "start_time": "2024-04-10T21:41:08.085973Z"
    }
   },
   "id": "7cae0d727e5eeb0e",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "465158aa8a6ff4fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Finally, we define a function that executes our IR layer and our LLM summarization layer.  The function accepts a question and queries it against our ChromaDB, retrieving a defined number of documents based on the smallest distance from the query.  The results are joined together and sent to the LLM as context along with the original question, to generate a summarized result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f82af0295f612e01",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:48:16.362490Z",
     "start_time": "2024-04-10T21:48:16.358205Z"
    }
   },
   "outputs": [],
   "source": [
    "def ask_question(question: str, model: AutoModelForCausalLM = model, tokenizer: AutoTokenizer = tokenizer, collection: str = COLLECTION_NAME, n_docs: int = 3) -> str:\n",
    "    \n",
    "    # Find close documents in chromadb\n",
    "    collection = client.get_collection(collection)\n",
    "    results = collection.query(\n",
    "       query_texts=[question],\n",
    "       n_results=n_docs\n",
    "    )\n",
    "\n",
    "    # Collect the results in a context\n",
    "    context = \"\\n\".join([r for r in results['documents'][0]])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    <start_of_turn>user\n",
    "    You are an expert on statistics and its applications to People Analytics.  \n",
    "    Here is a question: {question}\\n\\n Answer it with reference to the following information and only using the following information: {context}.<end_of_turn>\n",
    "    <start_of_turn>model\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the answer using the LLM\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Return the generated answer\n",
    "    outputs = model.generate(**input_ids, max_new_tokens=512)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).split('model\\n', 1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff85228a02fabd2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Testing our RAG agent\n",
    "\n",
    "We test our agent by asking some questions where relevant information is known to exist in the IR layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a9e3c4ae9fa4358",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:49:06.001174Z",
     "start_time": "2024-04-10T21:48:20.723373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'    Sure, here is the answer to the question:\\n\\nThe text suggests that the recommended method for modeling ordered category outcomes is proportional odds logistic regression. This is because proportional odds models are easy to interpret and are commonly adopted in the field. However, it is important to note that proportional odds models have some underlying assumptions that should be checked before using them.\\n\\nTherefore, the recommended method for modeling ordered category outcomes is proportional odds logistic regression, but it is important to check the underlying assumptions of this model before using it.'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question(\"What method would you recommend I use to model ordered category outcomes and why?\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'    When using Proportional Odds regression, you should look out for the following:\\n\\n    * **Test of Proportional Odds Assumption:** Before running the model, you should test the proportional odds assumption to see if the model is appropriate for your data. If the assumption fails, you should consider alternative models for ordinal outcomes.\\n    * **Variable Removal:** If the assumption fails, you may consider removing variables that do not impact the outcome. However, whether or not you are comfortable doing this will depend very much on the impact on overall model fit.\\n    * **Alternative Models:** If you are not comfortable removing variables, you should consider alternative models for ordinal outcomes. The most common alternatives include models such as cumulative logit models, rank-based models, and threshold-based models.'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question('What should I look out for when using Proportional Odds regression?')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:53:32.031903Z",
     "start_time": "2024-04-10T21:52:13.624098Z"
    }
   },
   "id": "e9fe32bb17b50fe0",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c3f779739c0d979",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:54:43.957174Z",
     "start_time": "2024-04-10T21:54:33.637587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'    The text suggests that the most common type of modeling in people analytics is regression modeling.'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question('What type of modeling is most likely to add value in People Analytics?')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'    Sure, here is the explanation of the term \"inferential modeling\" as per the text:\\n\\n**Inferential modeling** is the process of learning about a relationship (or lack of relationship) between the data in $X$ and $y$ and using that to *describe* a relationship (or lack of relationship) between our constructs $\\\\mathscr{C}$ and our outcome $\\\\mathscr{O}$ that is valid to a high degree of statistical certainty on the population $\\\\mathscr{P}$. This process includes testing a proposed mathematical relationship, comparing that relationship against other proposed relationships, describing the relationship statistically, and determining whether the relationship (or certain elements of it) can be generalized from the sample set $S$ to the population $\\\\mathscr{P}$.'"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question('Can you please explain what is meant by the term \"inferential modeling\"?')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:55:57.243476Z",
     "start_time": "2024-04-10T21:54:48.623844Z"
    }
   },
   "id": "c29f365ec58498b7",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "\"    Sure, here is the answer to the question:\\n\\nThe term 'regression' originated from the Latin term 'regressio', which approximately means 'go back'. This term was first used by Francis Galton in the late 1800s to describe a statistical phenomenon that he discovered while researching the relationship between the heights of a population of almost 1000 children and the average height of their parents.\""
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question('Where did the term regression originate from?')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:56:45.828418Z",
     "start_time": "2024-04-10T21:56:05.621916Z"
    }
   },
   "id": "cd557ebc1aeae963",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'    Sure, here is the answer to the question:\\n\\nTo get started using R for regression modeling, follow these steps:\\n\\n1. Download and install the latest version of R from https://www.r-project.org/. Ensure that the version suits your operating system.\\n2. Download the latest version of the RStudio IDE from https://rstudio.com/products/rstudio/ and view the video on that page to familiarize yourself with its features.\\n3. Open RStudio and play around.\\n\\nThe initial stages of using R can be challenging, mostly due to the need to become familiar with how R understands, stores and processes data. Extensive trial and error is a learning necessity. Perseverance is important in these early stages, as well as an openness to seek help from others either in person or via online forums.'"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question('How do I get started using R for regression modeling?')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:58:09.884606Z",
     "start_time": "2024-04-10T21:56:52.454867Z"
    }
   },
   "id": "e7f7470a231416f1",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a1f9da6cea645d7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:58:55.649429Z",
     "start_time": "2024-04-10T21:58:18.037386Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'    Sure, here is the answer to the question:\\n\\nThe factors that determine statistical power are the effect size, the sample size, the level of significance, and the standard deviation of the population.\\n\\nIn order to achieve a statistically significant result, the observed effect size must be greater than a certain multiple of the standard error above the null hypothesis mean. This multiple is called $z_{\\\\alpha}$.'"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question('What factors determine statistical power?')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also test to ensure that questions are only answered based on content in the book."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9095e9d4f59b000"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e9f5d584b1378690"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca249c3e0f7e8664",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T21:59:15.849683Z",
     "start_time": "2024-04-10T21:59:05.321803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'    The text does not provide information about the standard model of Physics, therefore I cannot answer this question.'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_question('What is the standard model of Physics?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
