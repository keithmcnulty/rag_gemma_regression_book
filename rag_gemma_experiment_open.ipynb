{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0e5249c625994e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e0f094428efea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Retrieval-Augmented Generation (RAG) Using Google Gemma-7b-it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9722825b18d9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this notebook we will implement the following RAG architecture to create a customized LLM using text from my textbook **Handbook of Regression Modeling** and Google's Gemma-7b-it open source LLM:\n",
    "\n",
    "![RAG Architecture](rag_architecture.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b03658dd838d5e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will be building two components, an information-retrieval (IR) component and a generation component:\n",
    "* The IR component acts as a knowledge database of text files.  This database will be used to identify documents or text passages most relevant to the intent of the user query. Embeddings will be used to search this database and return the most relevant passages from the textbook.\n",
    "* The generation component will feed the results of the IR component into the Gemma LLM as context in order to generate an attempt at a comprehensive natural language response to the user prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed1be801881cd6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preparing a dataset from my textbook to work with Gemma-7b-it LLM\n",
    "\n",
    "My textbook is available open source, and its codebase is in an open Github repository.  We will connect to the repository and download the text of the 14 chapters and sections of the textbook, and create a Pandas dataframe with 14 rows and two columns, containing an ID-number for each chapter/section and the text of each chapter/section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd3a95ff95c043",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# chapters are Rmd files with the following names\n",
    "chapter_list = [\n",
    "    \"01-intro\",\n",
    "    \"02-basic_r\",\n",
    "    \"03-primer_stats\",\n",
    "    \"04-linear_regression\",\n",
    "    \"05-binomial_logistic_regression\",\n",
    "    \"06-multinomial_regression\",\n",
    "    \"07-ordinal_regression\",\n",
    "    \"08-hierarchical_data\",\n",
    "    \"09-survival_analysis\",\n",
    "    \"10-tidy_modeling\",\n",
    "    \"11-power_tests\",\n",
    "    \"12-further\",\n",
    "    \"13-solutions\",\n",
    "    \"14-bibliography\"\n",
    "]\n",
    "\n",
    "# create a function to obtain the text of each chapter\n",
    "def get_text(chapter: str) -> str:\n",
    "    # URL on the Github where the rmd files are stored\n",
    "    github_url = f\"https://raw.githubusercontent.com/keithmcnulty/peopleanalytics-regression-book/master/r/{chapter}.Rmd\"  \n",
    "    \n",
    "    result = requests.get(github_url)\n",
    "    return result.text\n",
    "\n",
    "# iterate over the chapter URLs and pull down the text content    \n",
    "book_text = []\n",
    "for chapter in chapter_list:\n",
    "    chapter_text = get_text(chapter)\n",
    "    book_text.append(chapter_text)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# write to a dataframe\n",
    "book_data = dict(chapter = list(range(14)), text = book_text)\n",
    "book_data = pd.DataFrame.from_dict(book_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a1a682fe90ec35d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b179c9d6bcdc9cb2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Most of these text documents are too long to fit into Gemma's context window, and so we will need to split them into smaller documents in a way that makes some sort of semantic sense.  \n",
    "\n",
    "We will use a Langchain transformer to do semantic splitting, with a chunk size of 1000 and a chunk overlap of 150. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e43e94c91caa2b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# semantically split chapters to a max length of 1000\n",
    "loader = DataFrameLoader(book_data, page_content_column=\"text\")\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "# examine a document to ensure it looks as we expect\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d19a3123e9bc1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generating embeddings and storing in a vector DB\n",
    "\n",
    "![Embeddings](embeddings.png)\n",
    "\n",
    "Since we will use embeddings for the IR component, we need to generate embeddings for our split dataset and then write those into a vector database to allow them to be searched.  We will use the *all-MiniLM-L6-v2 model* to generate the embeddings and the ChromaDB vector database to store them.  Vector Databases have limits to the number of documents that can be encoded in a single command, and so we will use a batch command just in case there are too many documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18358ebb839a25",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.utils.batch_utils import create_batches\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd8b8d2b3b4bc3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up the ChromaDB\n",
    "CHROMA_DATA_PATH = \"./chroma_data_regression_book/\"\n",
    "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
    "COLLECTION_NAME = \"regression_book_docs\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb1e0aeeac6899",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# enable the DB using Cosine Similarity as the distance metric\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBED_MODEL\n",
    ")\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_func,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d63b79fb40377f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write text chunks to DB in batches \n",
    "batches = create_batches(\n",
    "    api=client,\n",
    "    ids=[f\"{uuid.uuid4()}\" for i in range(len(docs))], \n",
    "    documents=[doc.page_content for doc in docs], \n",
    "    metadatas=[{'source': './handbook_of_regression_modeling', 'row': k} for k in range(len(docs))]\n",
    ")\n",
    "\n",
    "for batch in batches:\n",
    "    print(f\"Adding batch of size {len(batch[0])}\")\n",
    "    collection.add(ids=batch[0],\n",
    "                   documents=batch[3],\n",
    "                   metadatas=batch[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c54d6d4810edee",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now our documents are persisting in our vector database, we can try running a query against them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b34776f4ec93aa9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"Which method would you recommend for ordered category outcomes?\"],\n",
    "    n_results=3, \n",
    "    include=['documents']\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also see that our language model creates an embedding of 384 dimensions for each document."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e3a1c88b42daf08"
  },
  {
   "cell_type": "markdown",
   "id": "6ae92b4efc3aa9c5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pipelining the RAG using ChromaDB and Gemma-7b\n",
    "\n",
    "We now have our vector DB in place so our IR layer is complete.  Now we will load the Gemma-7b LLM via Huggingface.  An access token is needed for this.  This model is large and will take some loading time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88bc879e5425f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print(\"MPS not available\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fe40f157bd70226",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892768b23836f5ac",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load model to Apple Silicon\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b-it\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b-it\", padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599083e8bfc4f824",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we have loaded the Gemma-7b-it model, we can set up an LLM pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can run the model normally and ask a general question to the Gemma model:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2591706286ae702"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<start_of_turn>user\n",
    "What food should I try in New Mexico?<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "# embed the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# generate the answer\n",
    "outputs = model.generate(**input_ids, max_new_tokens=512)\n",
    "\n",
    "# decode the answer\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True).split('model\\n', 1)[1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cae0d727e5eeb0e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "465158aa8a6ff4fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Finally, we define a function that executes our IR layer and our LLM summarization layer.  The function accepts a question and queries it against our ChromaDB, retrieving a defined number of documents based on the smallest distance from the query.  The results are joined together and sent to the LLM as context along with the original question, to generate a summarized result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82af0295f612e01",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ask_question(question: str, model: AutoModelForCausalLM = model, tokenizer: AutoTokenizer = tokenizer, collection: str = COLLECTION_NAME, n_docs: int = 3) -> str:\n",
    "    \n",
    "    # Find close documents in chromadb\n",
    "    collection = client.get_collection(collection)\n",
    "    results = collection.query(\n",
    "       query_texts=[question],\n",
    "       n_results=n_docs\n",
    "    )\n",
    "\n",
    "    # Collect the results in a context\n",
    "    context = \"\\n\".join([r for r in results['documents'][0]])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    <start_of_turn>user\n",
    "    You are an expert on statistics and its applications to People Analytics.  \n",
    "    Here is a question: {question}\\n\\n Answer it with reference to the following information and only using the following information: {context}.<end_of_turn>\n",
    "    <start_of_turn>model\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the answer using the LLM\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Return the generated answer\n",
    "    outputs = model.generate(**input_ids, max_new_tokens=512)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).split('model\\n', 1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff85228a02fabd2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Testing our RAG agent\n",
    "\n",
    "We test our agent by asking some questions where relevant information is known to exist in the IR layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e3c4ae9fa4358",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ask_question(\"What method would you recommend I use to model ordered category outcomes and why?\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ask_question('What should I look out for when using Proportional Odds regression?')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9fe32bb17b50fe0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f779739c0d979",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ask_question('What type of modeling is most likely to add value in People Analytics?')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ask_question('Can you please explain what is meant by the term \"inferential modeling\"?')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c29f365ec58498b7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ask_question('Where did the term regression originate from?')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd557ebc1aeae963",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ask_question('How do I get started using R for regression modeling?')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7f7470a231416f1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f9da6cea645d7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ask_question('What factors determine statistical power?')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also test to ensure that questions are only answered based on content in the book."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9095e9d4f59b000"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e9f5d584b1378690"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca249c3e0f7e8664",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ask_question('What is the standard model of Physics?')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "71dfbcdb758558f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
